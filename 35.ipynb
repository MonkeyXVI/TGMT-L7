{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pickle\n",
    "import os\n",
    " \n",
    "# Define the local path for the project files\n",
    "path = './'\n",
    " \n",
    "# Load models and setup\n",
    "faceDetectionModel = os.path.join(path, '/thigiacmaytinh/models/res10_300x300_ssd_iter_140000_fp16.caffemodel')\n",
    "faceDetectionProto = os.path.join(path, '/thigiacmaytinh/models/deploy.proto.txt')\n",
    "faceDescriptor = os.path.join(path, '/thigiacmaytinh/models/openface.nn4.small2.v1.t7')\n",
    "face_recognition_model = pickle.load(open(os.path.join(path, 'ml_face_person_identity.pkl'), 'rb'))\n",
    " \n",
    "# Load the face detection and descriptor models\n",
    "detectorModel = cv2.dnn.readNetFromCaffe(faceDetectionProto, faceDetectionModel)\n",
    "descriptorModel = cv2.dnn.readNetFromTorch(faceDescriptor)\n",
    " \n",
    "def pipeline_model(frame):\n",
    "    \"\"\"Process a frame and return the annotated frame and recognition results.\"\"\"\n",
    "    image = frame.copy()\n",
    "    h, w = frame.shape[:2]\n",
    "    # Face detection\n",
    "    img_blob = cv2.dnn.blobFromImage(frame, 1, (300, 300), (104, 177, 123), swapRB=False, crop=False)\n",
    "    detectorModel.setInput(img_blob)\n",
    "    detections = detectorModel.forward()\n",
    " \n",
    "    # Machine learning results\n",
    "    machinelearning_results = {\n",
    "        'face_detect_score': [],\n",
    "        'face_name': [],\n",
    "        'face_name_score': [],\n",
    "        'count': []\n",
    "    }\n",
    "    count = 1\n",
    " \n",
    "    # Process each detected face\n",
    "    for i, confidence in enumerate(detections[0, 0, :, 2]):\n",
    "        if confidence > 0.5:\n",
    "            # Get the bounding box for the face\n",
    "            box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "            startx, starty, endx, endy = box.astype(int)\n",
    " \n",
    "            cv2.rectangle(image, (startx, starty), (endx, endy), (0, 255, 0), 2)\n",
    " \n",
    "            # Extract face ROI\n",
    "            face_roi = frame[starty:endy, startx:endx]\n",
    "            face_blob = cv2.dnn.blobFromImage(face_roi, 1 / 255, (96, 96), (0, 0, 0), swapRB=True, crop=True)\n",
    "            descriptorModel.setInput(face_blob)\n",
    "            vectors = descriptorModel.forward()\n",
    " \n",
    "            # Predict face identity\n",
    "            face_name = face_recognition_model.predict(vectors)[0]\n",
    " \n",
    "            # Calculate face_name_score based on classifier agreement (for hard voting)\n",
    "            individual_predictions = [clf.predict(vectors)[0] for clf in face_recognition_model.estimators_]\n",
    "            agreement_count = sum([1 for pred in individual_predictions if pred == face_name])\n",
    "            total_classifiers = len(face_recognition_model.estimators_)\n",
    "            face_score = agreement_count / total_classifiers\n",
    " \n",
    "            # Display the predicted name and score on the image\n",
    "            text_face = '{} : {:.0f}%'.format(face_name, 100 * face_score)\n",
    "            cv2.putText(image, text_face, (startx, starty - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n",
    " \n",
    "            # Save results in dictionary\n",
    "            machinelearning_results['count'].append(count)\n",
    "            machinelearning_results['face_detect_score'].append(confidence)\n",
    "            machinelearning_results['face_name'].append(face_name)\n",
    "            machinelearning_results['face_name_score'].append(face_score)\n",
    "            count += 1\n",
    " \n",
    "    return image, machinelearning_results\n",
    " \n",
    "# Start video capture from the webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    " \n",
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    # Process the current frame\n",
    "    annotated_frame, results = pipeline_model(frame)\n",
    " \n",
    "    # Display the annotated frame\n",
    "    cv2.imshow('Webcam Face Recognition', annotated_frame)\n",
    " \n",
    "    # Press 'q' to quit the webcam stream\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    " \n",
    "# Release the capture and close windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
